import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from torch.utils.data import DataLoader, TensorDataset
import time

# บันทึกเวลาเริ่มต้นของโปรแกรมทั้งหมด
start_time = time.time()

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the data
df = pd.read_excel('project_data_003.xlsx')

# Define input and target variables
X = df[['month', 'building', 'Area', 'Eusers', 'Eusers-1', 'Eusers-2', 'Eusers-3', 'Eusers-4', 'Eusers-5',
        'Eusers-6', 'Eusers-7', 'Eusers-8', 'Eusers-9', 'Eusers-10', 'Eusers-11', 'exam', 'exam-1',
        'exam-2', 'exam-3', 'exam-4', 'exam-5', 'exam-6', 'exam-7', 'exam-8', 'exam-9', 'exam-10',
        'exam-11', 'semester', 'semester-1', 'semester-2', 'semester-3', 'semester-4', 'semester-5',
        'semester-6', 'semester-7', 'semester-8', 'semester-9', 'semester-10', 'semester-11', 'Holiday', 'Holiday-1',
        'Holiday-2', 'Holiday-3', 'Holiday-4', 'Holiday-5', 'Holiday-6', 'Holiday-7', 'Holiday-8', 'Holiday-9',
        'Holiday-10', 'Holiday-11', 'Unit', 'Unit-1', 'Unit-2', 'Unit-3', 'Unit-4', 'Unit-5', 'Unit-6', 'Unit-7',
        'Unit-8', 'Unit-9', 'Unit-10', 'Unit-11']].values

target_columns = ['Unit+1', 'Unit+2', 'Unit+3', 'Unit+4', 'Unit+5', 'Unit+6', 'Unit+7', 'Unit+8', 'Unit+9', 'Unit+10',
                  'Unit+11', 'Unit+12']

# Define GRU model
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # สำหรับ GRU เราจะได้รับ output และ hidden state เท่านั้น
        out, hn = self.gru(x)
        out = self.fc(hn[-1])
        return out

# Wrapper class for sklearn compatibility using GRU
class GRUWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, hidden_size=64, num_layers=2, learning_rate=0.001, dropout=0.2, num_epochs=150, batch_size=48):
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        self.dropout = dropout
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.model = None
        self.scaler_X = MinMaxScaler()
        self.scaler_y = MinMaxScaler()

    def fit(self, X, y):
        # Scale features and target
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

        # Convert to PyTorch tensors without sending them to GPU
        X_tensor = torch.FloatTensor(X_scaled).unsqueeze(1).to(device)
        y_tensor = torch.FloatTensor(y_scaled).unsqueeze(1).to(device)
        dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, pin_memory=False)

        # Initialize GRU model and move it to device
        self.model = GRUModel(
            input_size=X.shape[1],
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            output_size=1,
            dropout=self.dropout
        ).to(device)

        criterion = nn.SmoothL1Loss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

        # Training loop
        self.model.train()
        for epoch in range(self.num_epochs):
            for X_batch, y_batch in train_loader:
                # Move batch data to device here
                X_batch = X_batch.to(device)
                y_batch = y_batch.to(device)

                optimizer.zero_grad()
                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()
        return self

    def predict(self, X):
        self.model.eval()
        with torch.no_grad():
            X_scaled = self.scaler_X.transform(X)
            X_tensor = torch.FloatTensor(X_scaled).unsqueeze(1).to(device)
            y_pred = self.model(X_tensor).cpu().numpy()
            y_pred = self.scaler_y.inverse_transform(y_pred)
        return y_pred.flatten()

# Parameter grid for GridSearchCV
param_grid = {
    'hidden_size': [64, 128, 256],
    'num_layers': [2, 3, 4],
    'learning_rate': [0.001, 0.0005, 0.0001, 0.002, 0.0003],
    'dropout': [0.0, 0.1, 0.2, 0.3]
}

# Store all results
all_results = []

# For each target column
for target in target_columns:
    target_start_time = time.time()

    print(f"\nProcessing target: {target}")
    print('----------------------------------------------------------------------------')

    # Define the target variable for the current iteration
    y = df[target].values

    # Handle anomalies before splitting
    if np.any(np.isnan(y)) or np.any(np.isinf(y)):
        print(f"Found {np.sum(np.isnan(y))} NaN values and {np.sum(np.isinf(y))} Inf values")
        y = np.nan_to_num(y, nan=0, posinf=1e6, neginf=-1e6)

    # Splitting the data before scaling
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=28)

    # Perform GridSearchCV
    grid_search = GridSearchCV(
        GRUWrapper(),
        param_grid,
        cv=10,
        scoring='r2',
        verbose=1,
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    # Get best model and make predictions
    best_model = grid_search.best_estimator_
    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # Calculate metrics using original scale data
    epsilon = 1e-8
    metrics = {
        'Target': target,
        'Total Data': len(y),
        'Max (All)': y.max(),
        'Min (All)': y.min(),
        'Average (All)': y.mean(),
        'Sum (All)': y.sum(),
        'Train Data': len(y_train),
        'Max (Train)': y_train.max(),
        'Min (Train)': y_train.min(),
        'Average (Train)': y_train.mean(),
        'Sum (Train)': y_train.sum(),
        'Test Data': len(y_test),
        'Max (Test)': y_test.max(),
        'Min (Test)': y_test.min(),
        'Average (Test)': y_test.mean(),
        'Sum (Test)': y_test.sum(),
        'RMSE (Train)': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'MAE (Train)': np.mean(np.abs(y_train - y_train_pred)),
        'R^2 (Train)': r2_score(y_train, y_train_pred),
        'Pearson Correlation Coefficient (Train)': np.corrcoef(y_train, y_train_pred)[0, 1],
        'MAPE (Train, %)': np.mean(np.abs((y_train - y_train_pred) / (y_train + epsilon))) * 100,
        'RMSE (Test)': np.sqrt(mean_squared_error(y_test, y_test_pred)),
        'MAE (Test)': np.mean(np.abs(y_test - y_test_pred)),
        'R^2 (Test)': r2_score(y_test, y_test_pred),
        'Pearson Correlation Coefficient (Test)': np.corrcoef(y_test, y_test_pred)[0, 1],
        'MAPE (Test, %)': np.mean(np.abs((y_test - y_test_pred) / (y_test + epsilon))) * 100,
        'Best Hyperparameters': grid_search.best_params_,
        'Runtime for Target (seconds)': time.time() - target_start_time
    }

    all_results.append(metrics)

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best R2 Score: {grid_search.best_score_:.4f}")
    print(f"Test RMSE: {metrics['RMSE (Test)']:.4f}")
    print(f"Training set size: {len(X_train)}")
    print(f"Runtime: {metrics['Runtime for Target (seconds)']:.2f} seconds")

# Save results to CSV
results_df = pd.DataFrame(all_results)
results_df.to_csv('gru_results_improved_003.csv', index=False)

# บันทึกเวลาสิ้นสุดของโปรแกรมทั้งหมด
end_time = time.time()
print(f"\nTotal Runtime for all targets: {end_time - start_time:.2f} seconds")
