import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import optuna
from sklearn.base import BaseEstimator, RegressorMixin
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
import time

# Record overall start time
start_time = time.time()

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the data
df = pd.read_excel('project_data_003.xlsx')

# Define input and target variables
X = df[['month', 'building', 'Area', 'Eusers', 'Eusers-1', 'Eusers-2', 'Eusers-3', 'Eusers-4', 'Eusers-5',
        'Eusers-6', 'Eusers-7', 'Eusers-8', 'Eusers-9', 'Eusers-10', 'Eusers-11', 'exam', 'exam-1',
        'exam-2', 'exam-3', 'exam-4', 'exam-5', 'exam-6', 'exam-7', 'exam-8', 'exam-9', 'exam-10',
        'exam-11', 'semester', 'semester-1', 'semester-2', 'semester-3', 'semester-4', 'semester-5',
        'semester-6', 'semester-7', 'semester-8', 'semester-9', 'semester-10', 'semester-11', 'Holiday', 'Holiday-1',
        'Holiday-2', 'Holiday-3', 'Holiday-4', 'Holiday-5', 'Holiday-6', 'Holiday-7', 'Holiday-8', 'Holiday-9',
        'Holiday-10', 'Holiday-11', 'Unit', 'Unit-1', 'Unit-2', 'Unit-3', 'Unit-4', 'Unit-5', 'Unit-6', 'Unit-7',
        'Unit-8', 'Unit-9', 'Unit-10', 'Unit-11']].values

target_columns = ['Unit+1', 'Unit+2', 'Unit+3', 'Unit+4', 'Unit+5', 'Unit+6', 'Unit+7', 'Unit+8', 'Unit+9', 'Unit+10',
                  'Unit+11', 'Unit+12']

# =============================================================================
# Define GRU model
# =============================================================================
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out, hn = self.gru(x)
        out = self.fc(hn[-1])
        return out

# =============================================================================
# Updated Wrapper class for sklearn compatibility using GRU
# =============================================================================
class GRUWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, hidden_size=64, num_layers=2, learning_rate=0.001, dropout=0.2,
                 num_epochs=150, batch_size=48, scaler_X=None, scaler_y=None):
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        self.dropout = dropout
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.model = None
        # Use externally provided scalers if available; otherwise, instantiate new ones.
        self.scaler_X = scaler_X if scaler_X is not None else MinMaxScaler()
        self.scaler_y = scaler_y if scaler_y is not None else MinMaxScaler()

    def fit(self, X, y):
        # Scale features and target
        X_scaled = self.scaler_X.fit_transform(X)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

        # Convert to PyTorch tensors and add sequence dimension
        X_tensor = torch.FloatTensor(X_scaled).unsqueeze(1).to(device)
        y_tensor = torch.FloatTensor(y_scaled).unsqueeze(1).to(device)

        # Create DataLoader
        dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)

        # Initialize GRU model
        self.model = GRUModel(
            input_size=X.shape[1],
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            output_size=1,
            dropout=self.dropout
        ).to(device)

        # Loss and optimizer
        criterion = nn.SmoothL1Loss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

        # Training loop
        self.model.train()
        for epoch in range(self.num_epochs):
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        return self

    def predict(self, X):
        self.model.eval()
        with torch.no_grad():
            X_scaled = self.scaler_X.transform(X)
            X_tensor = torch.FloatTensor(X_scaled).unsqueeze(1).to(device)
            y_pred = self.model(X_tensor).cpu().numpy()
            y_pred = self.scaler_y.inverse_transform(y_pred)
        return y_pred.flatten()

global_scaler_X = MinMaxScaler()
global_scaler_y = MinMaxScaler()

# =============================================================================
# Hyperparameter tuning using Optuna for each target column
# =============================================================================
all_results = []

for target in target_columns:
    target_start_time = time.time()
    print(f"\nProcessing target: {target}")
    print('----------------------------------------------------------------------------')

    # Define the target variable
    y = df[target].values

    # Handle anomalies in target
    if np.any(np.isnan(y)) or np.any(np.isinf(y)):
        print(f"Found {np.sum(np.isnan(y))} NaN values and {np.sum(np.isinf(y))} Inf values")
        y = np.nan_to_num(y, nan=0, posinf=1e6, neginf=-1e6)

    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=28)

    # Define the objective function for Optuna
    def objective(trial):
        hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])
        num_layers = trial.suggest_categorical('num_layers', [2, 3, 4])
        learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.002, log=True)
        dropout = trial.suggest_float('dropout', 0.0, 0.3)
        num_epochs = trial.suggest_int('num_epochs', 100, 200)
        batch_size = trial.suggest_categorical('batch_size', [32, 48, 64])

        # Pass the global scalers to ensure consistency
        model = GRUWrapper(hidden_size=hidden_size, num_layers=num_layers,
                           learning_rate=learning_rate, dropout=dropout,
                           num_epochs=num_epochs, batch_size=batch_size,
                           scaler_X=global_scaler_X, scaler_y=global_scaler_y)

        # Further split the training data into train and validation sets
        X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
        model.fit(X_tr, y_tr)
        y_val_pred = model.predict(X_val)
        score = r2_score(y_val, y_val_pred)
        return score

    # Create and run the Optuna study (maximizing R^2)
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=30, timeout=600)  # adjust as needed

    best_params = study.best_trial.params
    best_score = study.best_trial.value
    print(f"Best params: {best_params}")
    print(f"Best Validation R2: {best_score:.4f}")

    # Train the best model on the full training set
    best_model = GRUWrapper(hidden_size=best_params['hidden_size'],
                            num_layers=best_params['num_layers'],
                            learning_rate=best_params['learning_rate'],
                            dropout=best_params['dropout'],
                            num_epochs=best_params['num_epochs'],
                            batch_size=best_params['batch_size'],
                            scaler_X=global_scaler_X, scaler_y=global_scaler_y)
    best_model.fit(X_train, y_train)

    # Make predictions on training and test sets
    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # Compute metrics
    epsilon = 1e-8
    metrics = {
        'Target': target,
        'Total Data': len(y),
        'Max (All)': y.max(),
        'Min (All)': y.min(),
        'Average (All)': y.mean(),
        'Sum (All)': y.sum(),
        'Train Data': len(y_train),
        'Max (Train)': y_train.max(),
        'Min (Train)': y_train.min(),
        'Average (Train)': y_train.mean(),
        'Sum (Train)': y_train.sum(),
        'Test Data': len(y_test),
        'Max (Test)': y_test.max(),
        'Min (Test)': y_test.min(),
        'Average (Test)': y_test.mean(),
        'Sum (Test)': y_test.sum(),
        'RMSE (Train)': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'MAE (Train)': np.mean(np.abs(y_train - y_train_pred)),
        'R^2 (Train)': r2_score(y_train, y_train_pred),
        'Pearson Correlation Coefficient (Train)': np.corrcoef(y_train, y_train_pred)[0, 1],
        'MAPE (Train, %)': np.mean(np.abs((y_train - y_train_pred) / (y_train + epsilon))) * 100,
        'RMSE (Test)': np.sqrt(mean_squared_error(y_test, y_test_pred)),
        'MAE (Test)': np.mean(np.abs(y_test - y_test_pred)),
        'R^2 (Test)': r2_score(y_test, y_test_pred),
        'Pearson Correlation Coefficient (Test)': np.corrcoef(y_test, y_test_pred)[0, 1],
        'MAPE (Test, %)': np.mean(np.abs((y_test - y_test_pred) / (y_test + epsilon))) * 100,
        'Best Hyperparameters': best_params,
        'Runtime for Target (seconds)': time.time() - target_start_time
    }

    all_results.append(metrics)
    print(f"Test RMSE: {metrics['RMSE (Test)']:.4f}")
    print(f"Training set size: {len(X_train)}")
    print(f"Runtime: {metrics['Runtime for Target (seconds)']:.2f} seconds")

# Save all results to CSV
results_df = pd.DataFrame(all_results)
results_df.to_csv('gru_optuna_results.csv', index=False)

# Print total runtime
end_time = time.time()
print(f"\nTotal Runtime for all targets: {end_time - start_time:.2f} seconds")
