import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from torch.utils.data import DataLoader, TensorDataset
import time

# บันทึกเวลาเริ่มต้นของโปรแกรมทั้งหมด
start_time = time.time()

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the data
df = pd.read_excel('project_data_003.xlsx')

# Define input and target variables
X = df[['month', 'building', 'Area', 'Eusers', 'Eusers-1', 'Eusers-2', 'Eusers-3', 'Eusers-4', 'Eusers-5',
        'Eusers-6', 'Eusers-7', 'Eusers-8', 'Eusers-9', 'Eusers-10', 'Eusers-11', 'exam', 'exam-1',
        'exam-2', 'exam-3', 'exam-4', 'exam-5', 'exam-6', 'exam-7', 'exam-8', 'exam-9', 'exam-10',
        'exam-11', 'semester', 'semester-1', 'semester-2', 'semester-3', 'semester-4', 'semester-5',
        'semester-6', 'semester-7', 'semester-8', 'semester-9', 'semester-10', 'semester-11', 'Holiday', 'Holiday-1',
        'Holiday-2', 'Holiday-3', 'Holiday-4', 'Holiday-5', 'Holiday-6', 'Holiday-7', 'Holiday-8', 'Holiday-9',
        'Holiday-10',
        'Holiday-11', 'Unit', 'Unit-1', 'Unit-2', 'Unit-3', 'Unit-4', 'Unit-5', 'Unit-6', 'Unit-7', 'Unit-8', 'Unit-9',
        'Unit-10', 'Unit-11']].values

target_columns = ['Unit+1', 'Unit+2', 'Unit+3', 'Unit+4', 'Unit+5', 'Unit+6', 'Unit+7', 'Unit+8', 'Unit+9', 'Unit+10',
                  'Unit+11', 'Unit+12']

# Define GRU model
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # GRU returns (output, hidden state)
        _, hn = self.gru(x)
        out = self.fc(hn[-1])
        return out

# Wrapper class for sklearn compatibility
class GRUWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, hidden_size=64, num_layers=2, learning_rate=0.001, dropout=0.2, num_epochs=100, batch_size=48):
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.learning_rate = learning_rate
        self.dropout = dropout
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.model = None
        self.scaler_y = MinMaxScaler()  # สเกลภายใน wrapper เท่านั้น

    def fit(self, X, y):
        # สเกลค่า y ภายในที่นี่ (y ที่รับเข้ามาจะเป็น raw value)
        y_scaled = self.scaler_y.fit_transform(y.reshape(-1, 1)).flatten()

        # แปลงข้อมูลเป็น PyTorch tensors พร้อมเพิ่ม dimension สำหรับ sequence length = 1
        X_tensor = torch.FloatTensor(X).unsqueeze(1).to(device)
        y_tensor = torch.FloatTensor(y_scaled).unsqueeze(1).to(device)

        # สร้าง DataLoader
        dataset = TensorDataset(X_tensor, y_tensor)
        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)

        # สร้าง model
        self.model = GRUModel(
            input_size=X.shape[1],
            hidden_size=self.hidden_size,
            num_layers=self.num_layers,
            output_size=1,
            dropout=self.dropout
        ).to(device)

        # Loss and optimizer
        criterion = nn.SmoothL1Loss()
        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)

        # Training loop
        self.model.train()
        for epoch in range(self.num_epochs):
            for X_batch, y_batch in train_loader:
                optimizer.zero_grad()
                outputs = self.model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        return self

    def predict(self, X):
        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).unsqueeze(1).to(device)
            y_pred = self.model(X_tensor).cpu().numpy()
            # นำผลลัพธ์กลับสู่สเกลของ y ดั้งเดิม
            y_pred = self.scaler_y.inverse_transform(y_pred)
            print(y_pred)
        return y_pred.flatten()


# Parameter grid for GridSearchCV
param_grid = {
    'hidden_size': [64, 128, 256],
    'num_layers': [2, 3, 4, 5],
    'learning_rate': [0.0001, 0.0002, 0.0003, 0.0005],
    'dropout': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
}

# Store all results
all_results = []

# สำหรับแต่ละ target column
for target in target_columns:
    target_start_time = time.time()

    print(f"\nProcessing target: {target}")
    print('----------------------------------------------------------------------------')

    # กำหนดตัวแปรเป้าหมายสำหรับการวนรอบแต่ละ target
    y = df[target].values

    # ตรวจสอบค่าผิดปกติ
    if np.any(np.isnan(y)) or np.any(np.isinf(y)):
        y = np.nan_to_num(y, nan=0, posinf=1e6, neginf=-1e6)

    # สเกลข้อมูลของ X เท่านั้น (ไม่สเกล y ที่นี่)
    scaler_X = MinMaxScaler()
    X_scaled = scaler_X.fit_transform(X)

    # แบ่งข้อมูลสำหรับการ train และ test (y ใช้ค่า raw)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, shuffle=True, random_state=28)

    # Perform GridSearchCV
    grid_search = GridSearchCV(
        GRUWrapper(),
        param_grid,
        cv=10,
        scoring='r2',
        verbose=1,
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)

    # รับ best model และทำการ predict
    best_model = grid_search.best_estimator_
    y_train_pred = best_model.predict(X_train)
    y_test_pred = best_model.predict(X_test)

    # คำนวณ metrics บนสเกลของ y ดั้งเดิม
    epsilon = 1e-8
    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
    r2_train = r2_score(y_train, y_train_pred)
    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
    r2_test = r2_score(y_test, y_test_pred)

    target_end_time = time.time()

    all_results.append({
        'Target': target,
        'Total Data': len(y),
        'Max (All)': y.max(),
        'Min (All)': y.min(),
        'Average (All)': y.mean(),
        'Sum (All)': y.sum(),
        'Train Data': len(y_train),
        'Max (Train)': y_train.max(),
        'Min (Train)': y_train.min(),
        'Average (Train)': y_train.mean(),
        'Sum (Train)': y_train.sum(),
        'Test Data': len(y_test),
        'Max (Test)': y_test.max(),
        'Min (Test)': y_test.min(),
        'Average (Test)': y_test.mean(),
        'Sum (Test)': y_test.sum(),
        'RMSE (Train)': rmse_train,
        'MAE (Train)': np.mean(np.abs(y_train - y_train_pred)),
        'R^2 (Train)': r2_train,
        'Pearson Correlation Coefficient (Train)': np.corrcoef(y_train, y_train_pred)[0, 1],
        'MAPE (Train, %)': np.mean(np.abs((y_train - y_train_pred) / (y_train + epsilon))) * 100,
        'RMSE (Test)': rmse_test,
        'MAE (Test)': np.mean(np.abs(y_test - y_test_pred)),
        'R^2 (Test)': r2_test,
        'Pearson Correlation Coefficient (Test)': np.corrcoef(y_test, y_test_pred)[0, 1],
        'MAPE (Test, %)': np.mean(np.abs((y_test - y_test_pred) / (y_test + epsilon))) * 100,
        'Best Hyperparameters': grid_search.best_params_,
        'Best R2 Score': grid_search.best_score_,
        'Runtime for Target (seconds)': target_end_time - target_start_time
    })

    print(f"Best parameters: {grid_search.best_params_}")
    print(f"Best R2 Score: {grid_search.best_score_:.4f}")
    print(f"Test RMSE: {rmse_test:.4f}")
    print(f"Runtime: {target_end_time - target_start_time:.2f} seconds")

# Save results to CSV
results_df = pd.DataFrame(all_results)
results_df.to_csv('gru_results_improved_03.csv', index=False)

# บันทึกเวลาสิ้นสุดของโปรแกรมทั้งหมด
end_time = time.time()
print(f"\nTotal Runtime for all targets: {end_time - start_time:.2f} seconds")
